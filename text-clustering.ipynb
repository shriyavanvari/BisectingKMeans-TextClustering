{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t3.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 20)\t1.0\n",
      "  (0, 22)\t1.0\n",
      "  (0, 30)\t5.0\n",
      "  (0, 35)\t1.0\n",
      "  (0, 41)\t1.0\n",
      "  (0, 46)\t1.0\n",
      "  (0, 49)\t1.0\n",
      "  (0, 61)\t1.0\n",
      "  (0, 69)\t1.0\n",
      "  (0, 78)\t1.0\n",
      "  (0, 85)\t1.0\n",
      "  (0, 105)\t1.0\n",
      "  (0, 117)\t2.0\n",
      "  (0, 121)\t1.0\n",
      "  (0, 135)\t1.0\n",
      "  (0, 142)\t1.0\n",
      "  (0, 145)\t2.0\n",
      "  (0, 169)\t4.0\n",
      "  (0, 175)\t3.0\n",
      "  (0, 205)\t1.0\n",
      "  (0, 206)\t1.0\n",
      "  (0, 210)\t1.0\n",
      "  (0, 262)\t3.0\n",
      "  :\t:\n",
      "  (8579, 7908)\t1.0\n",
      "  (8579, 8138)\t1.0\n",
      "  (8579, 8142)\t1.0\n",
      "  (8579, 8256)\t1.0\n",
      "  (8579, 10117)\t1.0\n",
      "  (8579, 10195)\t1.0\n",
      "  (8579, 10797)\t1.0\n",
      "  (8579, 11606)\t1.0\n",
      "  (8579, 11747)\t1.0\n",
      "  (8579, 12468)\t1.0\n",
      "  (8579, 12537)\t1.0\n",
      "  (8579, 12901)\t1.0\n",
      "  (8579, 13113)\t2.0\n",
      "  (8579, 13189)\t1.0\n",
      "  (8579, 13719)\t1.0\n",
      "  (8579, 13938)\t1.0\n",
      "  (8579, 13939)\t1.0\n",
      "  (8579, 19752)\t1.0\n",
      "  (8579, 21044)\t1.0\n",
      "  (8579, 22592)\t1.0\n",
      "  (8579, 25101)\t1.0\n",
      "  (8579, 27641)\t1.0\n",
      "  (8579, 32303)\t1.0\n",
      "  (8579, 49115)\t2.0\n",
      "  (8579, 85241)\t3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8580, 126356)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dealing with data in sparse format\n",
    "data = list()\n",
    "ilist = list()\n",
    "jlist = list()\n",
    "with open('train.dat', 'r') as fr:\n",
    "    for lineno, line in enumerate(fr):\n",
    "        line = line.strip()\n",
    "        #print(line)\n",
    "        parts = line.split()\n",
    "        #print(parts)\n",
    "        for i in range(int(len(parts)/2)):\n",
    "            # 1237 1 1390 1 1391 5 ...\n",
    "            jlist.append(int(parts[i*2]))\n",
    "            data.append(int(parts[i*2+1]))\n",
    "            ilist.append(lineno)\n",
    "\n",
    "cm = csr_matrix((data, (ilist, jlist)), dtype = np.float)\n",
    "print(cm)\n",
    "cm.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<8580x126356 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1107980 stored elements in Compressed Sparse Row format>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_initial = np.array(cm)\n",
    "X_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf scaling with normalization\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(norm=None, use_idf=True, smooth_idf=True)\n",
    "#transformer = TfidfTransformer(use_idf=True, smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from gensim.models import word2vec\\n\\n\\n# Set values for various parameters\\nfeature_size = 150    # Word vector dimensionality  \\nwindow_context = 5          # Context window size                                                                                    \\nmin_word_count = 1   # Minimum word count                        \\nsample = 1e-3   # Downsample setting for frequent words\\n\\nw2v_model = word2vec.Word2Vec(X_initial, size=feature_size, \\n            )'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from gensim.models import word2vec\n",
    "\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 150    # Word vector dimensionality  \n",
    "window_context = 5          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(X_initial, size=feature_size, \n",
    "            )'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8580, 126356)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmArr = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 ms, sys: 5.61 ms, total: 22.4 ms\n",
      "Wall time: 21.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=150)\n",
    "#svd.fit(cm.toarray())\n",
    "#print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#normalizer = Normalizer(copy=False)\n",
    "#lsa = make_pipeline(svd, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 34s, sys: 29.7 s, total: 3min 3s\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = svd.fit_transform(cmArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.decomposition import LatentDirichletAllocation\\nlda = LatentDirichletAllocation(n_components=150, max_iter=1, random_state=0)\\nX = lda.fit_transform(cmArr)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=150, max_iter=1, random_state=0)\n",
    "X = lda.fit_transform(cmArr)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8580, 150)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4218340023718873\n"
     ]
    }
   ],
   "source": [
    "# variance calculation\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a set of np.random K centroids for each feature of the given dataset\n",
    "def randCent(dataSet,K):\n",
    "    n=np.shape(dataSet)[1]\n",
    "    centroids=np.mat(np.zeros((K,n)))\n",
    "    for j in range(n):\n",
    "        minValue=min(dataSet[:,j])\n",
    "        maxValue=max(dataSet[:,j])\n",
    "        rangeValues=float(maxValue-minValue)\n",
    "        #Make sure centroids stay within the range of data\n",
    "        centroids[:,j]=minValue+rangeValues*np.random.rand(K,1)\n",
    "    # np.matrix\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean distance measure\n",
    "def distanceMeasure(vecOne, vecTwo):\n",
    "    return np.sqrt(np.sum(np.power(vecOne-vecTwo,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means clustering method\n",
    "def kMeans(dataSet,K,distMethods=distanceMeasure,createCent=randCent):\n",
    "    m=np.shape(dataSet)[0]\n",
    "    clusterAssment=np.mat(np.zeros((m,2)))\n",
    "    # np.matrix\n",
    "    centroids=createCent(dataSet,K)\n",
    "    clusterChanged=True\n",
    "    \n",
    "    while clusterChanged:\n",
    "        clusterChanged=False\n",
    "        for i in range(m):\n",
    "            minDist=np.inf; minIndex=-2\n",
    "            for j in range(K):\n",
    "                distJI=distMethods(centroids[j,:],dataSet[i,:])\n",
    "                if distJI<minDist:\n",
    "                    minDist=distJI;minIndex=j\n",
    "            if clusterAssment[i,0] != minIndex:\n",
    "                clusterChanged=True\n",
    "            clusterAssment[i,:]=minIndex,minDist**2\n",
    "        #update all the centroids by taking the np.mean value of relevant data\n",
    "        for cent in range(K):\n",
    "            ptsInClust=dataSet[np.nonzero(clusterAssment[:,0].A == cent)[0]]\n",
    "            centroids[cent,:]=np.mean(ptsInClust,axis=0)\n",
    "    # (np.matrix, np.matrix)\n",
    "    return centroids,clusterAssment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bisecting K-means method\n",
    "def bisectingKMeans(dataSet,K,numIterations):\n",
    "    m,n=dataSet.shape\n",
    "    clusterInformation = np.mat(np.zeros((m,2)))\n",
    "    centroidList=[]\n",
    "    minSSE = np.inf\n",
    "    \n",
    "    #At the first place, regard the whole dataset as a cluster and find the best clusters\n",
    "    for i in range(numIterations):\n",
    "        # (np.matrix, np.matrix)\n",
    "        centroid,clusterAssment=kMeans(dataSet, 2)\n",
    "        SSE=np.sum(clusterAssment,axis=0)[0,1]\n",
    "        if SSE<minSSE:\n",
    "            minSSE=SSE\n",
    "            tempCentroid=centroid\n",
    "            tempCluster=clusterAssment\n",
    "    centroidList.append(tempCentroid[0].tolist()[0])\n",
    "    centroidList.append(tempCentroid[1].tolist()[0])\n",
    "    clusterInformation=tempCluster\n",
    "    minSSE=np.inf \n",
    "    \n",
    "    while len(centroidList)<K:\n",
    "        maxIndex=-2\n",
    "        maxSSE=-1\n",
    "        #Choose the cluster with Maximum SSE to split\n",
    "        for j in range(len(centroidList)):\n",
    "            SSE=np.sum(clusterInformation[np.nonzero(clusterInformation[:,0]==j)[0]])\n",
    "            if SSE>maxSSE:\n",
    "                maxIndex=j\n",
    "                maxSSE=SSE\n",
    "                \n",
    "        #Choose the clusters with minimum total SSE to store into the centroidList\n",
    "        for k in range(numIterations):\n",
    "            pointsInCluster=dataSet[np.nonzero(clusterInformation[:,0]==maxIndex)[0]]\n",
    "            # (np.matrix, np.matrix)\n",
    "            centroid,clusterAssment=kMeans(pointsInCluster, 2)\n",
    "            SSE=np.sum(clusterAssment[:,1],axis=0)\n",
    "            if SSE<minSSE:\n",
    "                minSSE=SSE\n",
    "                tempCentroid=centroid.copy()\n",
    "                tempCluster=clusterAssment.copy()\n",
    "        #Update the index\n",
    "        tempCluster[np.nonzero(tempCluster[:,0]==1)[0],0]=len(centroidList)\n",
    "        tempCluster[np.nonzero(tempCluster[:,0]==0)[0],0]=maxIndex\n",
    "        \n",
    "        #update the information of index and SSE\n",
    "        clusterInformation[np.nonzero(clusterInformation[:,0]==maxIndex)[0],:]=tempCluster\n",
    "        #update the centrolist\n",
    "        centroidList[maxIndex]=tempCentroid[0].tolist()[0]\n",
    "        centroidList.append(tempCentroid[1].tolist()[0])\n",
    "    # (List[List[float]], np.matrix)\n",
    "    return centroidList,clusterInformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shriyavanvari/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/shriyavanvari/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "c_list, c_info = bisectingKMeans(X,7,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 7, list, 150, 201.09921568631754)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c_list), len(c_list), type(c_list[0]), len(c_list[0]), c_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "abc=\"\"\n",
    "with open('clustering_v11.0.txt', 'w') as fw:\n",
    "    print(\"ItemID,ClusterID\",file=fw)\n",
    "    for v in c_info[:, 0]+1:\n",
    "        abc=str(i)+\",\"+str(int(v.A[0][0]))\n",
    "        i=i+1\n",
    "        print(abc,file=fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"clustering_v11.0.txt\",delimiter=',')\n",
    "df.to_csv('Submission3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shriyavanvari/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/shriyavanvari/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/shriyavanvari/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/shriyavanvari/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [8580, 17160]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-867157f014a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalueK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msilhouetteScore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \"\"\"\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [8580, 17160]"
     ]
    }
   ],
   "source": [
    "# Using silhouette score as the metric\n",
    "valueK = list()\n",
    "silhouetteScore = list()\n",
    "labels = list()\n",
    "for k in range(3, 22, 2):\n",
    "    c_list, c_info = bisectingKMeans(X,k,10)\n",
    "    for v in c_info[:, 0]+1:\n",
    "        labels.append((int(v.A[0][0])))\n",
    "\n",
    "    valueK.append(k)\n",
    "    silhouetteScore.append(metrics.silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting silhoutte score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(valuek, silhouetteScore)\n",
    "plt.xlabel('Number of Clusters k')\n",
    "plt.ylabel('Silhouette Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
